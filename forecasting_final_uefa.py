# -*- coding: utf-8 -*-
"""Forecasting final UEFA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xmUDECeA3_0zjc_0yXe9vHku1g6iguyz

# **Forecasting the results of sports events**

# 1) Loading and preparation of the data
"""

import os
import subprocess
import logging
import sys
import warnings
import requests
import pandas as pd
from bs4 import BeautifulSoup

CSV_FILE = "matches_raw.csv"
DROPBOX_URL = (
    "https://www.dropbox.com/scl/fi/zk0zb58etdun5opkkucux/"
    "matches_raw.csv?rlkey=nu08nqynhu4odrjslvbf790jb&st=eycqa32t&dl=1"
)  # dl=1 ‚Üí direct download
USE_DROPBOX = False      # ‚Üê flip to True if you prefer the Dropbox snapshot

SEASON_URLS = {
    "2024-2025": "https://soccer365.ru/competitions/19/results/",
    "2023-2024": "https://soccer365.ru/competitions/19/2023-2024/results/",
    "2022-2023": "https://soccer365.ru/competitions/19/2022-2023/results/",
    "2021-2022": "https://soccer365.ru/competitions/19/2021-2022/results/",
    "2020-2021": "https://soccer365.ru/competitions/19/2020-2021/results/",
    "2019-2020": "https://soccer365.ru/competitions/19/2019-2020/results/",
    "2018-2019": "https://soccer365.ru/competitions/19/2018-2019/results/",
}

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/89.0.4389.114 Safari/537.36"
    )
}

# dual logger / printer
logging.basicConfig(
    stream=sys.stdout,
    level=logging.INFO,
    format="[{levelname:^7s} {asctime}] {message}",
    datefmt="%H:%M:%S",
    style="{",
)
logg = logging.getLogger("ucl_loader")
log = lambda m, lvl="info": (getattr(logg, lvl)(m), print(m))[1]


# helpers
def get_soup(url: str) -> BeautifulSoup:
    log(f"GET  {url}")
    r = requests.get(url, headers=HEADERS, timeout=20)
    r.raise_for_status()
    if "–õ–∏–≥–∞ —á–µ–º–ø–∏–æ–Ω–æ–≤ –£–ï–§–ê" not in r.text:
        raise ValueError("Unexpected page content (competition title not found).")
    return BeautifulSoup(r.text, "lxml")


def extract_match_links(results_url: str) -> list[str]:
    """Return sorted list of unique match URLs from a season results page."""
    soup = get_soup(results_url)
    base = "https://soccer365.ru"
    return sorted(
        {
            base + a["href"]
            for a in soup.select("a.game_link[href]")
            if "/games/" in a["href"]
        }
    )


def parse_match(url: str) -> dict:
    soup = get_soup(url)
    l, r = soup.select_one("div.live_game.left"), soup.select_one(
        "div.live_game.right"
    )
    return dict(
        URL=url,
        HomeTeam=l.a.text.strip(),
        HomeGoals=int(l.span.text.strip()),
        AwayTeam=r.a.text.strip(),
        AwayGoals=int(r.span.text.strip()),
    )


def scrape_matches() -> pd.DataFrame:
    """Scrape all seasons defined in SEASON_URLS and return DataFrame."""
    rows = []
    for season, url in SEASON_URLS.items():
        log(f"\n‚ïê‚ïê‚ïê‚ïê {season} ‚ïê‚ïê‚ïê‚ïê")
        for i, match_url in enumerate(extract_match_links(url), 1):
            log(f"[{i:03}] {match_url}")
            try:
                rows.append(parse_match(match_url))
            except Exception as e:
                warnings.warn(f"{match_url} -> {e}")
    df = pd.DataFrame(rows)
    log(f"‚úì  Scrape finished: {len(df)} matches collected.")
    return df


def download_csv(dropbox_url: str = DROPBOX_URL, dest: str = CSV_FILE) -> None:
    """Download CSV file from Dropbox."""
    log("Downloading CSV from Dropbox ‚Ä¶")
    try:
        subprocess.run(["wget", "-q", "-O", dest, dropbox_url], check=True)
        log("‚úì  CSV downloaded.")
    except Exception as e:
        raise RuntimeError(f"Failed to download CSV: {e}")


def load_matches_from_csv(csv_path: str = CSV_FILE) -> pd.DataFrame:
    """Load matches DataFrame from local CSV."""
    if not os.path.isfile(csv_path):
        raise FileNotFoundError(f"{csv_path} not found.")
    df = pd.read_csv(csv_path)
    log(f"‚úì  Loaded {len(df)} matches from {csv_path}")
    return df


def save_matches_to_csv(df: pd.DataFrame, csv_path: str = CSV_FILE) -> None:
    df.to_csv(csv_path, index=False)
    log(f"‚úì  Saved {len(df)} matches to {csv_path}")


try:
    if USE_DROPBOX:
        if not os.path.isfile(CSV_FILE):
            download_csv()
        matches_raw = load_matches_from_csv()
    else:
        matches_raw = scrape_matches()
        save_matches_to_csv(matches_raw)
except Exception as exc:
    log(f"‚ö†  An error occurred: {exc}", "error")
    raise

# Keep only essential columns
matches = matches_raw[["HomeTeam", "AwayTeam", "HomeGoals", "AwayGoals"]]
log(f"\nTotal matches in DataFrame: {len(matches)}")

matches.head()

matches_raw

"""# 2) Modelling and comparing of 3 chosen models"""

import numpy as np, pandas as pd, warnings, importlib, subprocess, sys
import statsmodels.formula.api as smf, statsmodels.api as sm
from scipy.stats import poisson
import xgboost as xgb
import ipywidgets as widgets
from IPython.display import display, clear_output


if importlib.util.find_spec("lightgbm") is None:
    print("Installing LightGBM ‚Ä¶ (one-time, 20-30 s)")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "lightgbm"])
import lightgbm as lgb


req = {"HomeTeam","AwayTeam","HomeGoals","AwayGoals"}
if not req.issubset(matches.columns):
    raise ValueError(f"`matches` missing {req - set(matches.columns)}")


gld = pd.concat([
    matches[["HomeTeam","AwayTeam","HomeGoals"]]
        .assign(home=1)
        .rename(columns={"HomeTeam":"team","AwayTeam":"opponent","HomeGoals":"goals"}),
    matches[["AwayTeam","HomeTeam","AwayGoals"]]
        .assign(home=0)
        .rename(columns={"AwayTeam":"team","HomeTeam":"opponent","AwayGoals":"goals"})
])
pois_mdl = smf.glm("goals ~ home + team + opponent", data=gld,
                   family=sm.families.Poisson()).fit()

def predict_pois(home, away, max_g=8):
    mu_h = pois_mdl.predict({"team":[home],"opponent":[away],"home":[1]})[0]
    mu_a = pois_mdl.predict({"team":[away],"opponent":[home],"home":[0]})[0]
    return _prob_table(mu_h, mu_a, max_g)

X_oh = pd.get_dummies(matches[["HomeTeam","AwayTeam"]]); X_oh["home"]=1
dmat = xgb.DMatrix(X_oh)
params = dict(objective="count:poisson", eval_metric="poisson-nloglik",
              eta=.05, max_depth=5, subsample=.8, colsample_bytree=.8)
xgb_h = xgb.train(params, xgb.DMatrix(X_oh, matches.HomeGoals), 600, verbose_eval=False)
xgb_a = xgb.train(params, xgb.DMatrix(X_oh, matches.AwayGoals), 600, verbose_eval=False)

def predict_xgb(home, away, max_g=8):
    vec = _vectorize(home, away, X_oh.columns)
    mu_h = float(xgb_h.predict(xgb.DMatrix(vec))[0])
    mu_a = float(xgb_a.predict(xgb.DMatrix(vec))[0])
    return _prob_table(mu_h, mu_a, max_g)


lgb_params = dict(objective="poisson", metric="poisson", learning_rate=.05,
                  max_depth=5, num_leaves=31, subsample=.8, colsample_bytree=.8,
                  verbose=-1)
lgb_h = lgb.train(lgb_params,
                  lgb.Dataset(X_oh, matches.HomeGoals),
                  num_boost_round=600)
lgb_a = lgb.train(lgb_params,
                  lgb.Dataset(X_oh, matches.AwayGoals),
                  num_boost_round=600)

def predict_lgb(home, away, max_g=8):
    vec = _vectorize(home, away, X_oh.columns)
    mu_h = float(lgb_h.predict(vec)[0])
    mu_a = float(lgb_a.predict(vec)[0])
    return _prob_table(mu_h, mu_a, max_g)


def _vectorize(home, away, cols):
    v = pd.get_dummies(pd.DataFrame({"HomeTeam":[home], "AwayTeam":[away]}))
    v["home"] = 1
    return v.reindex(columns=cols, fill_value=0)

def _prob_table(mu_h, mu_a, max_g):
    pmf_h = poisson.pmf(np.arange(max_g+1)[:,None], mu=mu_h)
    pmf_a = poisson.pmf(np.arange(max_g+1)[None,:],  mu=mu_a)
    mat   = pmf_h*pmf_a
    hw,dw,aw = np.tril(mat,-1).sum(), np.diag(mat).sum(), np.triu(mat,1).sum()
    exp_h = int(round((mat.sum(1)*np.arange(mat.shape[0])).sum()))
    exp_a = int(round((mat.sum(0)*np.arange(mat.shape[1])).sum()))
    return dict(HG=exp_h, AG=exp_a, HW=hw, D=dw, AW=aw)


clubs = sorted(matches.HomeTeam.unique())
home_dd  = widgets.Dropdown(options=clubs, description="Home:")
away_dd  = widgets.Dropdown(options=clubs, description="Away:")
model_dd = widgets.ToggleButtons(
    options=[("Poisson GLM","pois"), ("XGBoost Poisson","xgb"), ("LightGBM Poisson","lgb")],
    description="Model:")
out = widgets.Output()

def refresh(_=None):
    with out:
        clear_output()
        h, a, tag = home_dd.value, away_dd.value, model_dd.value
        if h == a: print("‚ö† choose two different clubs"); return
        pred = {"pois":predict_pois,
                "xgb": predict_xgb,
                "lgb": predict_lgb}[tag](h,a)

        winner = h if pred["HW"]==max(pred["HW"],pred["D"],pred["AW"]) else (
                 a if pred["AW"]==max(pred["HW"],pred["D"],pred["AW"]) else "Draw")

        display(pd.DataFrame({
            "HomeTeam":[h], "AwayTeam":[a],
            "HomeGoals":[pred["HG"]], "AwayGoals":[pred["AG"]],
            "HomeWinProb":[round(pred["HW"],6)],
            "DrawProb":[round(pred["D"],6)],
            "HomeLossProb":[round(pred["AW"],6)],
            "Winner":[winner],
            "Model":[model_dd.label]
        }))

for w in (home_dd, away_dd, model_dd):
    w.observe(refresh, names="value")

display(widgets.HBox([home_dd, away_dd, model_dd]), out)
refresh()

"""# 3) Comparison of models' quality by different metrcis to choose the BEST one"""

from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, log_loss)
import numpy as np, pandas as pd

def probs_poisson(row):           # returns [P(H), P(D), P(A)]
    pr = predict_pois(row.HomeTeam, row.AwayTeam)
    return np.array([pr["HW"], pr["D"], pr["AW"]])

def probs_xgb(row):
    pr = predict_xgb(row.HomeTeam, row.AwayTeam)
    return np.array([pr["HW"], pr["D"], pr["AW"]])

def probs_lgb(row):
    pr = predict_lgb(row.HomeTeam, row.AwayTeam)
    return np.array([pr["HW"], pr["D"], pr["AW"]])

PROB_FUNCS = {"Poisson GLM": probs_poisson,
              "XGBoost Poisson": probs_xgb,
              "LightGBM Poisson": probs_lgb}


y_true = np.where(matches.HomeGoals > matches.AwayGoals, 0,
         np.where(matches.HomeGoals < matches.AwayGoals, 2, 1))

rows = []
for name, fn in PROB_FUNCS.items():
    probs = np.vstack(matches.apply(fn, axis=1).to_numpy())
    y_pred = probs.argmax(axis=1)

    rows.append({
        "Model"     : name,
        "Accuracy"  : round(accuracy_score(y_true, y_pred), 3),
        "Precision" : round(precision_score(y_true, y_pred, average="macro"), 3),
        "Recall"    : round(recall_score(y_true, y_pred, average="macro"), 3),
        "F1-score"  : round(f1_score(y_true, y_pred, average="macro"), 3),
        "Log-loss"  : round(log_loss(y_true, probs), 3)
    })

score_table = pd.DataFrame(rows).set_index("Model").sort_values("F1-score", ascending=False)
display(score_table)

"""# 4) Visualisation"""

import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style="whitegrid")
palette = sns.color_palette("viridis", n_colors=len(score_table))

metrics = ["Accuracy", "Precision", "Recall", "F1-score"]

# graph 1: Comparison Accuracy / Precision / Recall / F1
fig1, ax1 = plt.subplots(figsize=(9, 5))
score_table[metrics].plot(kind="bar", ax=ax1, color=palette, edgecolor='black')

ax1.set_title("Model Comparison ‚Äì Accuracy / Precision / Recall / F1", fontsize=14)
ax1.set_ylabel("Score (0‚Äì1)", fontsize=12)
ax1.set_ylim(0, 1)
ax1.set_xticklabels(score_table.index, rotation=0, fontsize=12)

ax1.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0), fontsize=11)
plt.tight_layout()
plt.show()

# graph 2: Comparison Log-loss
fig2, ax2 = plt.subplots(figsize=(8, 4))
score_table["Log-loss"].plot(kind="bar", ax=ax2, color=palette, edgecolor='black')

ax2.set_title("Model Comparison ‚Äì Log-loss", fontsize=14)
ax2.set_ylabel("Log-loss", fontsize=12)
ax2.set_xticklabels(score_table.index, rotation=0, fontsize=12)

plt.tight_layout()
plt.show()

"""# **Working with news to discover whether the was an influence of SMM on the teams' mood and its' results**

This code analyzes how well sentiment from news headlines correlates with actual football match outcomes, using a comparison approach that can later include both VADER and BERT models.

# 1) VADER
"""

!pip install feedparser

"""This code collects recent news headlines about selected football clubs from Google News RSS feeds, analyzes the sentiment of each article using the VADER lexicon from the NLTK library, and stores the results in a structured CSV file. It supports loading a pre-saved dataset from Dropbox or scraping fresh data if no snapshot is available. All results are cached to Google Drive for later use. The final output is a dataframe containing team names, article titles, dates, links, sentiment scores, and text snippets ‚Äî ready for further analysis on how media coverage might influence team performance."""

from google.colab import drive
drive.mount("/content/drive", force_remount=False)
import os, sys, time, logging, html, re, requests
import datetime as dt
import feedparser
import pandas as pd
import nltk

nltk.download("vader_lexicon", quiet=True)
from nltk.sentiment import SentimentIntensityAnalyzer

NEWS_CACHE_PATH = "/content/drive/MyDrive/football_cache/team_news.csv"
os.makedirs(os.path.dirname(NEWS_CACHE_PATH), exist_ok=True)

DROPBOX_URL = (
    "https://www.dropbox.com/scl/fi/abcd1234abcd/team_news.csv?rlkey=xyz&dl=1"
)
USE_DROPBOX = False   # ‚Üê flip to True to load the Dropbox snapshot instead

# A list of clubs
if "matches" in globals():
    CLUBS = sorted(set(matches.HomeTeam) | set(matches.AwayTeam))
else:
    CLUBS = [
        "Real Madrid",
        "Manchester City",
        "Bayern Munich",
        "Paris Saint-Germain",
        "FC Barcelona",
    ]  # fallback sample ‚Äì edit as needed

# logger
logging.basicConfig(stream=sys.stdout,
                    level=logging.INFO,
                    format="[{levelname:^7s} {asctime}] {message}",
                    datefmt="%H:%M:%S", style='{')
logg = logging.getLogger("team_news")
log  = lambda m, lvl="info": (getattr(logg, lvl)(m), print(m))[1]

# utility functions
def download_csv(dropbox_url: str = DROPBOX_URL, dest: str = NEWS_CACHE_PATH) -> None:
    """Download a CSV snapshot from Dropbox."""
    log("Downloading CSV from Dropbox ‚Ä¶")
    import subprocess
    try:
        subprocess.run(["wget", "-q", "-O", dest, dropbox_url], check=True)
        log("‚úì  CSV downloaded.")
    except Exception as e:
        raise RuntimeError(f"Failed to download CSV: {e}")

def load_news_from_csv(csv_path: str = NEWS_CACHE_PATH) -> pd.DataFrame:
    if not os.path.isfile(csv_path):
        raise FileNotFoundError(f"{csv_path} not found.")
    df = pd.read_csv(csv_path, parse_dates=["date"])
    log(f"‚úì  Loaded {len(df)} headlines from {csv_path}")
    return df

def fetch_google_news(team: str, n_articles: int = 30, pause: float = 0.4) -> list[dict]:
    """Fetch recent headlines for one team from Google News RSS."""
    log(f"Fetching Google-News for {team} ‚Ä¶")
    url = (
        "https://news.google.com/rss/search?"
        f"q={requests.utils.quote(team+' football')}&hl=en&gl=US&ceid=US:en"
    )
    feed = feedparser.parse(url)
    rows = []
    for e in feed.entries[:n_articles]:
        txt = html.unescape(e.title + " " + e.get("summary", ""))
        rows.append(
            {
                "team": team,
                "title": e.title,
                "link": e.link,
                "date": dt.datetime(*e.published_parsed[:6]).date(),
                "sentiment": SIA.polarity_scores(txt)["compound"],
                "snippet": re.sub("<.*?>", "", e.get("summary", ""))[:200],
            }
        )
    log(f"‚Ä¶ {len(rows)} articles")
    time.sleep(pause)
    return rows

def scrape_news(teams: list[str]) -> pd.DataFrame:
    rows = []
    for club in teams:
        try:
            rows.extend(fetch_google_news(club))
        except Exception as e:
            log(f"‚ö† {club}: {e}", "error")
    df = pd.DataFrame(rows)
    log(f"TOTAL headlines collected: {len(df)}")
    return df

def save_news_to_csv(df: pd.DataFrame, csv_path: str = NEWS_CACHE_PATH) -> None:
    df.to_csv(csv_path, index=False)
    log(f"‚úì  Headlines cached to {csv_path}")

try:
    if USE_DROPBOX:
        if not os.path.isfile(NEWS_CACHE_PATH):
            download_csv()       # fetch once to Drive
        team_news = load_news_from_csv()
    else:
        SIA = SentimentIntensityAnalyzer()
        team_news = scrape_news(CLUBS)
        save_news_to_csv(team_news)
except Exception as exc:
    log(f"‚ö†  An error occurred: {exc}", "error")
    raise

log(f"\nFirst few rows:")
team_news.head()

!pip install dateparser

"""This code analyzes the relationship between recent news sentiment and football match outcomes for selected teams. It processes news articles to extract dates and sentiment scores, links them with actual match results, and identifies whether media tone aligns with team performance. For each team, it displays recent news, sentiment indicators, key words, and a verdict on whether the mood in the media likely influenced the match result. A dropdown menu allows switching between teams to explore these patterns interactively."""

import ipywidgets as widgets, numpy as np, pandas as pd, datetime as dt
import re, collections, dateparser
from IPython.display import display, clear_output
from nltk.sentiment import SentimentIntensityAnalyzer


if "team_news" not in globals() or "matches" not in globals():
    raise NameError("–ù—É–∂–Ω—ã DataFrame `team_news` –∏ `matches`.")


def ensure_news_date(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if "date" not in df.columns:
        df["date"] = pd.NaT
    miss = df.date.isna()
    if miss.any():
        months_ru = "—è–Ω–≤–∞—Ä—è —Ñ–µ–≤—Ä–∞–ª—è –º–∞—Ä—Ç–∞ –∞–ø—Ä–µ–ª—è –º–∞—è –∏—é–Ω—è –∏—é–ª—è –∞–≤–≥—É—Å—Ç–∞ —Å–µ–Ω—Ç—è–±—Ä—è –æ–∫—Ç—è–±—Ä—è –Ω–æ—è–±—Ä—è –¥–µ–∫–∞–±—Ä—è".split()
        months_en = ("january february march april may june july august "
                     "september october november december").split()
        pat = re.compile(r"(\d{1,2}\s+(?:"+"|".join(months_ru+months_en)+r")\s+\d{4})", re.I)
        df.loc[miss,"date"] = df.loc[miss,"title"].apply(
            lambda t: (d.date() if (m:=pat.search(t)) and (d:=dateparser.parse(m[1], languages=["ru","en"]))
                       else pd.NaT))
    return df

team_news = ensure_news_date(team_news)


has_dates = "Date" in matches.columns
if has_dates:
    matches["Date"] = pd.to_datetime(matches["Date"]).dt.date
    matches["gd_home"] = matches.HomeGoals - matches.AwayGoals


tok_pat = re.compile(r"[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø–ê-–Ø–∞-—è]+")
sia = SentimentIntensityAnalyzer()

def top_words(texts: list[str], overall: float, k: int = 5) -> list[str]:
    if not texts:
        return ["<–Ω–µ—Ç>"]
    sign = 1 if overall >= 0 else -1
    cnt  = collections.Counter()
    for txt in texts:
        for tok in tok_pat.findall(txt.lower()):
            v = sia.lexicon.get(tok)
            if v: cnt[tok] += v
    filt = {w:v for w,v in cnt.items() if v*sign > 0} or cnt
    return [w for w,_ in sorted(filt.items(), key=lambda kv: abs(kv[1]), reverse=True)][:k] or ["<–Ω–µ—Ç>"]


daily_sent = (team_news.groupby(["team","date"])["sentiment"]
                        .mean().rename("avg_sent").reset_index())


club_dd = widgets.Dropdown(options=sorted(matches.HomeTeam.unique()),
                           description="–ö–ª—É–±:")
out = widgets.Output()

def refresh(_=None):
    with out:
        clear_output()
        club = club_dd.value

        news = team_news[(team_news.team == club) & team_news.date.notna()].copy()
        if news.empty:
            print(f"–ù–µ—Ç –¥–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è ¬´{club}¬ª."); return
        news = news.sort_values("date", ascending=False)

        gm = matches[(matches.HomeTeam == club) | (matches.AwayTeam == club)]
        if gm.empty:
            print(f"–£ ¬´{club}¬ª –Ω–µ—Ç –º–∞—Ç—á–µ–π."); return
        gm_sort = gm if not has_dates else gm.sort_values("Date")


        last = gm.iloc[-1]
        is_home = last.HomeTeam == club
        gd = (last.HomeGoals - last.AwayGoals) if is_home else (last.AwayGoals - last.HomeGoals)
        outcome = "–ü–æ–±–µ–¥–∞" if gd > 0 else ("–ü–æ—Ä–∞–∂–µ–Ω–∏–µ" if gd < 0 else "–ù–∏—á—å—è")

        ref_date = last.Date if has_dates else dt.date.today()
        win_dates = [ref_date - dt.timedelta(d) for d in (1, 2, 3)]

        # —Ç–µ–∫—Å—Ç—ã –∏–∑ –æ–∫–Ω–∞ (title + snippet)
        def concat_row(r):
            snip = r.snippet if ("snippet" in r and pd.notna(r.snippet)) else ""
            return f"{r.title} {snip}"
        win_texts = [concat_row(r) for _, r in news.iterrows() if r.date in win_dates]

        avg_sent = (daily_sent[(daily_sent.team == club) &
                               (daily_sent.date.isin(win_dates))]["avg_sent"].mean()
                    if win_texts else 0)
        aligned = (avg_sent > 0 and gd > 0) or (avg_sent < 0 and gd < 0)
        verdict = "‚úì –ö–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç" if aligned else "√ó –ë–µ–∑ –≤–ª–∏—è–Ω–∏—è"
        has_inf = "W" if aligned else ("L" if abs(avg_sent) >= 0.15 else "N")
        summary_kw = ", ".join(top_words(win_texts, avg_sent))


        if has_dates:
            link = pd.merge_asof(news.sort_values("date"),
                                 gm_sort[["Date","HomeTeam","AwayTeam","HomeGoals","AwayGoals"]]
                                     .rename(columns={"Date":"date"}).sort_values("date"),
                                 on="date", direction="backward")

            def res(r):
                if pd.isna(r.HomeGoals):
                    return pd.Series([pd.NA, pd.NA])
                short = f"{r.HomeTeam} {int(r.HomeGoals)}-{int(r.AwayGoals)} {r.AwayTeam}"
                long  = (f"{club}: {r.AwayTeam}  {int(r.HomeGoals)}:{int(r.AwayGoals)}"
                         if r.HomeTeam == club else
                         f"{club}: {r.HomeTeam}  {int(r.AwayGoals)}:{int(r.HomeGoals)}")
                return pd.Series([short, long])
            link[["–ü–æ—Å–ª–µ–¥–Ω–∏–π –º–∞—Ç—á", "–†–µ–∑—É–ª—å—Ç–∞—Ç"]] = link.apply(res, axis=1)
        else:
            link = news.copy()
            link[["–ü–æ—Å–ª–µ–¥–Ω–∏–π –º–∞—Ç—á", "–†–µ–∑—É–ª—å—Ç–∞—Ç"]] = "<–Ω–µ—Ç –¥–∞—Ç—ã>"

        # –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        link["–°–ª–æ–≤–∞-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã"] = link.title.apply(
            lambda t: ", ".join(top_words([t], sia.polarity_scores(t)["compound"])) )

        link["–í–µ—Ä–¥–∏–∫—Ç"] = verdict
        link["has_influenced"] = has_inf

        cols = ["date", "title", "sentiment", "–í–µ—Ä–¥–∏–∫—Ç", "–°–ª–æ–≤–∞-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã",
                "has_influenced", "–ü–æ—Å–ª–µ–¥–Ω–∏–π –º–∞—Ç—á", "–†–µ–∑—É–ª—å—Ç–∞—Ç"]
        print(f"–ù–æ–≤–æ—Å—Ç–∏ –¥–ª—è ¬´{club}¬ª (10 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö):")
        display(link[cols].head(10))

        summary = pd.DataFrame({
            "–ö–ª—É–±":                [club],
            "–†–µ–∑—É–ª—å—Ç–∞—Ç":           [link['–†–µ–∑—É–ª—å—Ç–∞—Ç'].dropna().iloc[0]
                                    if link['–†–µ–∑—É–ª—å—Ç–∞—Ç'].notna().any() else "<–Ω–µ—Ç –¥–∞—Ç—ã>"],
            "–ò—Å—Ö–æ–¥":               [outcome],
            "–°—Ä–µ–¥–Ω. —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å":  [round(avg_sent, 3)],
            "–í–µ—Ä–¥–∏–∫—Ç":             [verdict],
            "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞":      [summary_kw],
            "has_influenced":      [has_inf]
        })
        display(summary)

club_dd.observe(refresh, names="value")
display(club_dd, out)
refresh()

import ipywidgets as widgets, numpy as np, pandas as pd, datetime as dt
import re, collections, dateparser
from IPython.display import display, clear_output
from nltk.sentiment import SentimentIntensityAnalyzer


if "team_news" not in globals() or "matches" not in globals():
    raise NameError("–ù—É–∂–Ω—ã DataFrame `team_news` –∏ `matches`.")

def ensure_news_date(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if "date" not in df.columns:
        df["date"] = pd.NaT
    miss = df.date.isna()
    if miss.any():
        months_ru = "—è–Ω–≤–∞—Ä—è —Ñ–µ–≤—Ä–∞–ª—è –º–∞—Ä—Ç–∞ –∞–ø—Ä–µ–ª—è –º–∞—è –∏—é–Ω—è –∏—é–ª—è –∞–≤–≥—É—Å—Ç–∞ —Å–µ–Ω—Ç—è–±—Ä—è –æ–∫—Ç—è–±—Ä—è –Ω–æ—è–±—Ä—è –¥–µ–∫–∞–±—Ä—è".split()
        months_en = ("january february march april may june july august "
                     "september october november december").split()
        pat = re.compile(r"(\d{1,2}\s+(?:"+"|".join(months_ru+months_en)+r")\s+\d{4})", re.I)
        df.loc[miss,"date"] = df.loc[miss,"title"].apply(
            lambda t: (d.date() if (m:=pat.search(t)) and (d:=dateparser.parse(m[1], languages=["ru","en"]))
                       else pd.NaT))
    return df

team_news = ensure_news_date(team_news)


has_dates = "Date" in matches.columns
if has_dates:
    matches["Date"] = pd.to_datetime(matches["Date"]).dt.date
    matches["gd_home"] = matches.HomeGoals - matches.AwayGoals


tok_pat = re.compile(r"[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø–ê-–Ø–∞-—è]+")
sia = SentimentIntensityAnalyzer()

def top_words(texts: list[str], overall: float, k: int = 5) -> list[str]:
    if not texts:
        return ["<–Ω–µ—Ç>"]
    sign = 1 if overall >= 0 else -1
    cnt  = collections.Counter()
    for txt in texts:
        for tok in tok_pat.findall(txt.lower()):
            v = sia.lexicon.get(tok)
            if v: cnt[tok] += v
    filt = {w:v for w,v in cnt.items() if v*sign > 0} or cnt
    return [w for w,_ in sorted(filt.items(), key=lambda kv: abs(kv[1]), reverse=True)][:k] or ["<–Ω–µ—Ç>"]


daily_sent = (team_news.groupby(["team","date"])["sentiment"]
                        .mean().rename("avg_sent").reset_index())


club_dd = widgets.Dropdown(options=sorted(matches.HomeTeam.unique()),
                           description="–ö–ª—É–±:")
out = widgets.Output()

def refresh(_=None):
    with out:
        clear_output()
        club = club_dd.value

        news = team_news[(team_news.team == club) & team_news.date.notna()].copy()
        if news.empty:
            print(f"–ù–µ—Ç –¥–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è ¬´{club}¬ª."); return
        news = news.sort_values("date", ascending=False)

        gm = matches[(matches.HomeTeam == club) | (matches.AwayTeam == club)]
        if gm.empty:
            print(f"¬´–£ {club}¬ª –Ω–µ—Ç –º–∞—Ç—á–µ–π."); return
        gm_sort = gm if not has_dates else gm.sort_values("Date")


        last = gm.iloc[-1]
        is_home = last.HomeTeam == club
        gd = (last.HomeGoals - last.AwayGoals) if is_home else (last.AwayGoals - last.HomeGoals)
        outcome = "Win" if gd > 0 else ("Loss" if gd < 0 else "Draw")

        ref_date = last.Date if has_dates else dt.date.today()
        win_dates = [ref_date - dt.timedelta(d) for d in (1, 2, 3)]

        # —Ç–µ–∫—Å—Ç—ã –∏–∑ –æ–∫–Ω–∞ (title + snippet)
        def concat_row(r):
            snip = r.snippet if ("snippet" in r and pd.notna(r.snippet)) else ""
            return f"{r.title} {snip}"
        win_texts = [concat_row(r) for _, r in news.iterrows() if r.date in win_dates]

        avg_sent = (daily_sent[(daily_sent.team == club) &
                               (daily_sent.date.isin(win_dates))]["avg_sent"].mean()
                    if win_texts else 0)
        aligned = (avg_sent > 0 and gd > 0) or (avg_sent < 0 and gd < 0)
        verdict = "‚úÖ Is correlated" if aligned else "‚ùå No influence"
        has_inf = "W" if aligned else ("L" if abs(avg_sent) >= 0.15 else "N")
        summary_kw = ", ".join(top_words(win_texts, avg_sent))


        if has_dates:
            link = pd.merge_asof(news.sort_values("date"),
                                 gm_sort[["Date","HomeTeam","AwayTeam","HomeGoals","AwayGoals"]]
                                     .rename(columns={"Date":"date"}).sort_values("date"),
                                 on="date", direction="backward")

            def res(r):
                if pd.isna(r.HomeGoals):
                    return pd.Series([pd.NA, pd.NA])
                short = f"{r.HomeTeam} {int(r.HomeGoals)}-{int(r.AwayGoals)} {r.AwayTeam}"
                long  = (f"{club}: {r.AwayTeam}  {int(r.HomeGoals)}:{int(r.AwayGoals)}"
                         if r.HomeTeam == club else
                         f"{club}: {r.HomeTeam}  {int(r.AwayGoals)}:{int(r.HomeGoals)}")
                return pd.Series([short, long])
            link[["–ü–æ—Å–ª–µ–¥–Ω–∏–π –º–∞—Ç—á", "–†–µ–∑—É–ª—å—Ç–∞—Ç"]] = link.apply(res, axis=1)
        else:
            link = news.copy()
            link[["–ü–æ—Å–ª–µ–¥–Ω–∏–π –º–∞—Ç—á", "–†–µ–∑—É–ª—å—Ç–∞—Ç"]] = "<–Ω–µ—Ç –¥–∞—Ç—ã>"

        # –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        link["–°–ª–æ–≤–∞-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã"] = link.title.apply(
            lambda t: ", ".join(top_words([t], sia.polarity_scores(t)["compound"])) )

        link["–í–µ—Ä–¥–∏–∫—Ç"] = verdict
        link["has_influenced"] = has_inf

        cols = ["date", "title", "sentiment", "–í–µ—Ä–¥–∏–∫—Ç", "–°–ª–æ–≤–∞-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã",
                "has_influenced", "–ü–æ—Å–ª–µ–¥–Ω–∏–π –º–∞—Ç—á", "–†–µ–∑—É–ª—å—Ç–∞—Ç"]
        print(f"–ù–æ–≤–æ—Å—Ç–∏ –¥–ª—è ¬´{club}¬ª (10 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö):")
        display(link[cols].head(10))

        summary = pd.DataFrame({
            "–ö–ª—É–±":                [club],
            "–†–µ–∑—É–ª—å—Ç–∞—Ç":           [link['–†–µ–∑—É–ª—å—Ç–∞—Ç'].dropna().iloc[0]
                                    if link['–†–µ–∑—É–ª—å—Ç–∞—Ç'].notna().any() else "<–Ω–µ—Ç –¥–∞—Ç—ã>"],
            "–ò—Å—Ö–æ–¥":               [outcome],
            "–°—Ä–µ–¥–Ω. —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å":  [round(avg_sent, 3)],
            "–í–µ—Ä–¥–∏–∫—Ç":             [verdict],
            "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞":      [summary_kw],
            "has_influenced":      [has_inf]
        })
        display(summary)

club_dd.observe(refresh, names="value")
display(club_dd, out)
refresh()

team_news['team'].unique()

team_news.query("team=='–ë–∞—Ä—Å–µ–ª–æ–Ω–∞'")

team_news.query("team=='–ë–∞—Ä—Å–µ–ª–æ–Ω–∞'")['link'].iloc[0]

matches_raw

team_news

"""This code connects football match results with news sentiment data by matching each news article to the closest previous match date using asof merge . It then analyzes whether the sentiment of the article aligns with the team‚Äôs performance in that match ‚Äî checking if positive/negative tone corresponds to wins/losses. The script identifies key emotional words in headlines, labels whether media mood likely influenced the result, and displays the most emotionally charged positive and negative news articles with their impact assessment."""

import pandas as pd, numpy as np, re, collections, datetime as dt, dateparser
from IPython.display import display
from nltk.sentiment import SentimentIntensityAnalyzer


news = team_news.copy()

if "date" not in news.columns:
    news["date"] = pd.NaT
news["date"] = pd.to_datetime(news["date"], errors="coerce")

def guess_date(t: str):
    pat = re.compile(r"(\d{1,2}\s+(?:—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|"
                     r"–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è|"
                     r"january|february|march|april|may|june|july|august|"
                     r"september|october|november|december)\s+\d{4})", re.I)
    m = pat.search(t)
    return dateparser.parse(m[1], languages=['ru','en']) if m else None

news.loc[news.date.isna(), "date"] = news.loc[news.date.isna(), "title"].apply(guess_date)
news = news.dropna(subset=["date", "sentiment"]).copy()
news["date"] = pd.to_datetime(news["date"]).astype("datetime64[ns]")


matches_cp = matches.copy()
if "Date" not in matches_cp.columns:
    matches_cp["Date"] = pd.Timestamp(dt.datetime.now())
matches_cp["Date"] = pd.to_datetime(matches_cp["Date"]).astype("datetime64[ns]")

games = (matches_cp.sort_values("Date")
         [["Date","HomeTeam","AwayTeam","HomeGoals","AwayGoals"]]
         .rename(columns={"Date":"match_date"}))


linked = pd.merge_asof(
            news.sort_values("date"),
            games.sort_values("match_date"),
            left_on="date",
            right_on="match_date",
            direction="backward")


tok_pat = re.compile(r"[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø–ê-–Ø–∞-—è]+")
sia     = SentimentIntensityAnalyzer()
THR     = 0.15

def top_tokens(text, score, k=5):
    sign = 1 if score >= 0 else -1
    cnt  = collections.Counter()
    for tok in tok_pat.findall(text.lower()):
        v = sia.lexicon.get(tok);  cnt[tok] += v if v else 0
    filt = {w:v for w,v in cnt.items() if v*sign > 0} or cnt
    return ", ".join([w for w,_ in sorted(filt.items(),
                                          key=lambda kv:abs(kv[1]), reverse=True)][:k]) or "<no>"

def res_strings(r, club):
    if pd.isna(r.HomeGoals):
        return "<not found>", "<not found>"
    short = f"{r.HomeTeam} {int(r.HomeGoals)}-{int(r.AwayGoals)} {r.AwayTeam}"
    long  = (f"{club}: {r.AwayTeam}  {int(r.HomeGoals)}:{int(r.AwayGoals)}"
             if r.HomeTeam == club else
             f"{club}: {r.HomeTeam}  {int(r.AwayGoals)}:{int(r.HomeGoals)}")
    return short, long

def enrich(row):
    club = row.team
    row["–ü–æ—Å–ª–µ–¥–Ω–∏–π –º–∞—Ç—á"], row["–†–µ–∑—É–ª—å—Ç–∞—Ç"] = res_strings(row, club)
    if "<–º–∞—Ç—á" in row["–ü–æ—Å–ª–µ–¥–Ω–∏–π –º–∞—Ç—á"]:
        row["–í–µ—Ä–¥–∏–∫—Ç"] = "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"; row["has_influenced"] = "N"
    else:
        gd = (row.HomeGoals-row.AwayGoals) if row.HomeTeam == club else (row.AwayGoals-row.HomeGoals)
        align = abs(row.sentiment) >= THR and ((row.sentiment>0 and gd>0) or (row.sentiment<0 and gd<0))
        row["–í–µ—Ä–¥–∏–∫—Ç"] = "‚úì –≤–ª–∏—è–µ—Ç" if align else "√ó –Ω–µ –≤–ª–∏—è–µ—Ç"
        row["has_influenced"] = "W" if align else ("L" if abs(row.sentiment) >= THR else "N")
    row["–°–ª–æ–≤–∞-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã"] = top_tokens(row.title, row.sentiment)
    return row

linked = linked.apply(enrich, axis=1)
linked["abs_sent"] = linked.sentiment.abs()

cols = ["date","title","sentiment","–í–µ—Ä–¥–∏–∫—Ç","–°–ª–æ–≤–∞-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã",
        "has_influenced","–ü–æ—Å–ª–µ–¥–Ω–∏–π –º–∞—Ç—á","–†–µ–∑—É–ª—å—Ç–∞—Ç"]


top_pos = (linked[linked.sentiment > 0]
           .sort_values("abs_sent", ascending=False)
           .head(5)[cols])

top_neg = (linked[linked.sentiment < 0]
           .sort_values("abs_sent", ascending=False)
           .head(5)[cols])

print("üîù‚úÖ 5 POSITIVE:")
display(top_pos.style.format({"sentiment":"{:+.3f}"}))

print("\nüîù‚ùå 5 NEGATIVE:")
display(top_neg.style.format({"sentiment":"{:+.3f}"}))

"""# 2) BERT-model"""

"""
‚úì –ú–æ–¥–µ–ª—å:  distilbert-base-multilingual-cased-finetuned-sentiment
‚úì –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç score ‚àà [-1,1]  —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å —Ç–µ–∫—É—â–µ–π —Å—Ö–µ–º–æ–π
"""
!pip install -q transformers sentencepiece

from transformers import pipeline
import numpy as np

bert_sa = pipeline("sentiment-analysis",
                   model="nlptown/bert-base-multilingual-uncased-sentiment",
                   top_k=None)

linked

linked.columns

"""This code explores the relationship between the sentiment of football-related news headlines and actual match outcomes, with the goal of determining whether media tone can reflect or even predict team performance. It starts by preparing and cleaning two main data sources: news articles linked to the nearest past match for each mentioned team, and match results transformed into a long format so that each team‚Äôs performance (goals scored, goals conceded, venue) is represented individually. For each news article, the script identifies the most recent match played by the associated team and appends relevant performance metrics such as goals scored and conceded, and calculates the goal difference as an indicator of match outcome (win, loss, or draw).

To understand which words in the headlines are most indicative of positive or negative outcomes, the code extracts keywords and builds token-level statistics across all headlines with known results. It computes win rate, loss rate, and net score for each keyword, filtering out rare or irrelevant terms. These scores are then aggregated into a custom headline sentiment metric (tok_score) based on the predictive power of its constituent words. This provides a domain-specific sentiment signal tailored to football news.

The script also prepares numeric versions of all relevant features ‚Äî including VADER sentiment, keyword strength, and match results ‚Äî and computes correlations between them. A correlation matrix is generated and visualized using a heatmap to show how strongly each feature relates to match outcomes. This creates a framework for comparing different sentiment analysis models, such as VADER and BERT, by evaluating which model's sentiment scores align more closely with real-world match results. The ultimate aim is to assess whether advanced NLP models like BERT offer a meaningful improvement over simpler methods in capturing the link between media tone and sports performance.
"""

import pandas as pd, numpy as np, matplotlib.pyplot as plt
from collections import Counter

MIN_APPEAR = 3
UNKNOWN_KEYWORDS = "<–Ω–µ—Ç>"
HOME_COL, AWAY_COL = "HomeGoals", "AwayGoals"   # from matches_raw

#  News (linked) "keywords" column
news = linked.copy()
if "keywords" not in news.columns and "–°–ª–æ–≤–∞-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã" in news.columns:
    news.rename(columns={"–°–ª–æ–≤–∞-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã": "keywords"}, inplace=True)
news["keywords"] = news["keywords"].fillna(UNKNOWN_KEYWORDS).astype(str)

# Results (matches_raw)
home = (matches_raw
        .assign(team=matches_raw["HomeTeam"],
                opp = matches_raw["AwayTeam"],
                team_goals = matches_raw[HOME_COL],
                opp_goals  = matches_raw[AWAY_COL],
                venue="home")
        [["team","opp","team_goals","opp_goals","venue"]])

away = (matches_raw
        .assign(team=matches_raw["AwayTeam"],
                opp = matches_raw["HomeTeam"],
                team_goals = matches_raw[AWAY_COL],
                opp_goals  = matches_raw[HOME_COL],
                venue="away")
        [["team","opp","team_goals","opp_goals","venue"]])

matches_long = pd.concat([home, away], ignore_index=True)

# latest match per team (could swap to earliest or any other rule)
latest_match = (matches_long
                .drop_duplicates(subset="team", keep="last")
                .set_index("team"))

# news with score info
def fetch_latest_result(team: str):
    """Return team_goals, opp_goals for most recent match, else (NaN, NaN)."""
    if team in latest_match.index:
        r = latest_match.loc[team]
        return pd.Series([r.team_goals, r.opp_goals])
    return pd.Series([np.nan, np.nan])

news[["team_goals", "opp_goals"]] = news["team"].apply(fetch_latest_result)

# numeric coercion
news[["team_goals", "opp_goals"]] = news[["team_goals", "opp_goals"]].apply(
    pd.to_numeric, errors="coerce"
)

# keyword length
news["kw_len"] = (
    news["keywords"].str.count(",").fillna(0).astype(int)
    + (news["keywords"] != UNKNOWN_KEYWORDS).astype(int)
)

# goal difference from team‚Äôs perspective
news["goal_diff"] = news["team_goals"] - news["opp_goals"]

news["result_num"] = (
    news["goal_diff"].apply(lambda x: np.sign(x) if pd.notna(x) else pd.NA)
    .astype("Int8")
)

news["aligned_num"] = (
    news.get("has_influenced")
        .map({"W": 1, "L": -1})
        .astype("Int8")
)

# win/loss Stat
tok_win, tok_loss, tok_total = Counter(), Counter(), Counter()

for _, row in news.dropna(subset=["result_num"]).iterrows():
    toks = [t.strip() for t in row["keywords"].split(",")
            if t.strip() and t.strip() != UNKNOWN_KEYWORDS]
    tok_total.update(toks)
    if row["result_num"] == 1:
        tok_win.update(toks)
    elif row["result_num"] == -1:
        tok_loss.update(toks)

rows = []
for tok, total in tok_total.items():
    if total < MIN_APPEAR:
        continue
    w, l = tok_win[tok], tok_loss[tok]
    rows.append(dict(
        token = tok, appearances = total, wins = w, losses = l,
        win_rate = round(w/total, 3),
        loss_rate = round(l/total, 3),
        win_minus_loss = round((w-l)/total, 3)
    ))

if rows:
    tok_df = (pd.DataFrame(rows)
              .sort_values("win_minus_loss", ascending=False, na_position="last")
              .reset_index(drop=True))
else:
    print(f"No token met MIN_APPEAR = {MIN_APPEAR}.")
    tok_df = pd.DataFrame(columns=[
        "token","appearances","wins","losses",
        "win_rate","loss_rate","win_minus_loss"
    ])

print("\n=== Top tokens associated with wins vs losses ===")
print(tok_df.head(20).to_string(index=False))

# map token ‚Üí win_minus_loss, default 0
tok_score_map = tok_df.set_index("token")["win_minus_loss"].to_dict()

def headline_token_score(kw: str) -> float:
    toks = [t.strip() for t in kw.split(",")
            if t.strip() and t.strip() != UNKNOWN_KEYWORDS]
    return float(np.mean([tok_score_map.get(t, 0) for t in toks])) if toks else 0.0

news["tok_score"] = news["keywords"].apply(headline_token_score)

#  Corr matrix (robust)
numeric_cols = ["sentiment","abs_sent","kw_len","tok_score",
                "aligned_num","goal_diff","result_num"]

news[numeric_cols] = news[numeric_cols].apply(pd.to_numeric, errors="coerce").astype(float)

# Keep rows that have at least TWO finite numbers (corr needs pairs)
corr_input = news[numeric_cols].dropna(thresh=2)

if corr_input.empty:
    print("Cannot build a correlation matrix")
    corr = pd.DataFrame(index=numeric_cols, columns=numeric_cols)
else:
    corr = corr_input.corr(min_periods=2).round(3)   # min_periods avoids NaNs from too-few pairs

print("\nCorrelation matrix (forced numeric)")
print(corr.fillna("").to_string())

# nice heat-map if we have anything meaningful
if corr_input.empty or corr.isna().all().all():
    print("\nNothing to plot ‚Äì every correlation is NaN.")
else:
    fig, ax = plt.subplots(figsize=(7,6))
    im = ax.imshow(corr, vmin=-1, vmax=1, cmap="coolwarm")
    ax.set_xticks(range(len(corr)), labels=corr.columns, rotation=45, ha="right")
    ax.set_yticks(range(len(corr)), labels=corr.index)
    for i in range(len(corr)):
        for j in range(len(corr)):
            ax.text(j, i, f"{corr.iat[i,j]:+.2f}" if pd.notna(corr.iat[i,j]) else "",
                    ha="center", va="center", fontsize=8)
    ax.set_title("Correlation matrix")
    fig.colorbar(im, shrink=0.8)
    plt.tight_layout()
    plt.show()

"""# **Comparison with Poisson**

This code scrapes detailed match data from the UEFA Champions League on soccer365.ru, including scores, statistics, and stadium info. It builds a structured dataset for analysis or machine learning, with support for resuming interrupted scrapes or loading a pre-saved snapshot from Dropbox. The final output is a clean CSV file with numeric values and enriched features like home advantage and attendance estimates.
"""

USE_DRIVE   = False
USE_DROPBOX = False
START_FROM  = 0

if USE_DRIVE:
    try:
        from google.colab import drive
        drive.mount("/content/drive", force_remount=False)
        DRIVE_ROOT = "/content/drive/MyDrive"
        print("Google Drive mounted ‚úì")
    except Exception:
        DRIVE_ROOT = "."
        print("Drive mount failed ‚Äî saving locally")
else:
    DRIVE_ROOT = "."
    print("Google Drive disabled ‚Äî saving locally")

import os, re, time, random, warnings, requests, unicodedata
import numpy as np, pandas as pd
from bs4 import BeautifulSoup as bs
warnings.filterwarnings("ignore")

COMP_ID = "19"
BASE    = "https://soccer365.ru"

SEASON_URLS = {
    "2024-2025": f"{BASE}/competitions/{COMP_ID}/results/",
    "2023-2024": f"{BASE}/competitions/{COMP_ID}/2023-2024/results/",
    "2022-2023": f"{BASE}/competitions/{COMP_ID}/2022-2023/results/",
    "2021-2022": f"{BASE}/competitions/{COMP_ID}/2021-2022/results/",
    "2020-2021": f"{BASE}/competitions/{COMP_ID}/2020-2021/results/",
    "2019-2020": f"{BASE}/competitions/{COMP_ID}/2019-2020/results/",
    "2018-2019": f"{BASE}/competitions/{COMP_ID}/2018-2019/results/",
}

SAVE_PATH   = os.path.join(DRIVE_ROOT, "final_uefa.csv")
DROPBOX_URL = ("https://www.dropbox.com/scl/fi/abcd1234abcd/final_uefa.csv"
               "?rlkey=xyz&dl=1")

HEADERS      = {"User-Agent": "Mozilla/5.0"}
STADIUMS_URL = f"{BASE}/competitions/{COMP_ID}/stadiums/"

STAT_COLS = [
    "xG","–£–¥–∞—Ä—ã","–£–¥–∞—Ä—ã –≤ —Å—Ç–≤–æ—Ä","–ë–ª–æ–∫-–Ω–æ —É–¥–∞—Ä–æ–≤","–°–µ–π–≤—ã","–í–ª–∞–¥–µ–Ω–∏–µ %","–£–≥–ª–æ–≤—ã–µ",
    "–ù–∞—Ä—É—à–µ–Ω–∏—è","–û—Ñ—Å–∞–π–¥—ã","–ñ–µ–ª—Ç—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏","–ö—Ä–∞—Å–Ω—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏","–ê—Ç–∞–∫–∏",
    "–û–ø–∞—Å–Ω—ã–µ –∞—Ç–∞–∫–∏","–ü–µ—Ä–µ–¥–∞—á–∏","–¢–æ—á–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–¥–∞—á %","–®—Ç—Ä–∞—Ñ–Ω—ã–µ —É–¥–∞—Ä—ã",
    "–í–±—Ä–∞—Å—ã–≤–∞–Ω–∏—è","–ù–∞–≤–µ—Å—ã"
]

# helpers
def _get(url, tries=3, pause=1):
    for a in range(tries):
        try:
            r = requests.get(url, headers=HEADERS, timeout=20); r.raise_for_status()
            return bs(r.content, "html.parser")
        except Exception:
            if a == tries-1: raise
            time.sleep(pause + random.random())

def all_match_links():
    """Unique list of match URLs from *all* SEASON_URLS."""
    links = set()
    for season, url in SEASON_URLS.items():
        print(f"Collecting links for {season} ‚Ä¶")
        soup = _get(url)
        links.update(BASE + a["href"]
                     for a in soup.select("a.game_link[href]")
                     if "/games/" in a["href"])
    return sorted(links)

def safe_int(txt):
    if txt is None: return np.nan
    t = unicodedata.normalize("NFKD", txt).strip()
    return np.nan if t in {"", "-", "‚Äì"} else int(t)

def parse_match(url, idx, total):
    print(f"[{idx:04}/{total}] {url}")
    s  = _get(url)
    t1 = s.select_one("div.live_game.left  a").text.strip()
    g1 = safe_int(s.select_one("div.live_game.left  span").text)
    t2 = s.select_one("div.live_game.right a").text.strip()
    g2 = safe_int(s.select_one("div.live_game.right span").text)
    d  = dict(–ö–æ–º–∞–Ω–¥–∞_1=t1, –ì–æ–ª—ã_1=g1, –ö–æ–º–∞–Ω–¥–∞_2=t2, –ì–æ–ª—ã_2=g2, URL=url)

    for blk in s.select("div.stats_item"):
        title = blk.select_one("div.stats_title").text.strip()
        vals  = [float(re.sub(r"[^\d.\-]", "", x.text)) if x.text.strip() else np.nan
                 for x in blk.select("div.stats_inf")]
        d[f"{title}_1"], d[f"{title}_2"] = vals

    header = s.select_one("#game_events h2").text
    rnd  = re.search(r"(\d+)[\-\s]*—Ç—É—Ä", header, re.I) \
        or re.search(r"–†–∞—É–Ω–¥\s*(\d+)", header, re.I)
    d["–†–∞—É–Ω–¥"] = int(rnd.group(1)) if rnd else np.nan
    dt = re.search(r"\d{2}\.\d{2}\.\d{4}", header)
    tm = re.search(r"\d{2}:\d{2}", header)
    d["–î–∞—Ç–∞"] = pd.to_datetime(dt.group(0), dayfirst=True) if dt else pd.NaT
    d["–í—Ä–µ–º—è"] = tm.group(0) if tm else np.nan

    odds = s.select("table.adv_kef_wgt tr.adv_kef_wgt_odd td span.koeff")
    if odds: d["–ö—ç—Ñ_1"], d["–ö—ç—Ñ_—Ö"], d["–ö—ç—Ñ_2"] = [float(o.text) for o in odds[:3]]

    prev = s.select_one("#preview div.block_body")
    if prev:
        st = prev.select_one("div.preview_item.st")
        if st:
            a = st.select_one("a");    d["–°—Ç–∞–¥–∏–æ–Ω"] = a.text.strip() if a else np.nan
            t = st.select_one("div.img16.weath_tmp span.red")
            d["–ì—Ä–∞–¥—É—Å—ã"] = t.text.strip() if t else np.nan
            w = st.select("span.min_gray")
            d["–ü–æ–≥–æ–¥–∞"] = w[1].text.strip() if len(w) > 1 else np.nan
        for it in prev.select("div.preview_item"):
            if "–ó—Ä–∏—Ç–µ–ª–µ–π" in it.text:
                d["–ó—Ä–∏—Ç–µ–ª–∏"] = int(re.sub(r"[^\d]", "", it.text)); break

    for k in ["–ó—Ä–∏—Ç–µ–ª–∏","–ü–æ–≥–æ–¥–∞","–ì—Ä–∞–¥—É—Å—ã","–°—Ç–∞–¥–∏–æ–Ω","–ö—ç—Ñ_1","–ö—ç—Ñ_—Ö","–ö—ç—Ñ_2"]:
        d.setdefault(k, np.nan)
    for c in STAT_COLS:
        d.setdefault(f"{c}_1", np.nan); d.setdefault(f"{c}_2", np.nan)
    return d

# stadium helpers (unchanged from earlier snippet)
def stadium_table():
    s = _get(STADIUMS_URL)
    rows = []
    for tr in s.select("tr")[1:]:
        td = tr.find_all("td")
        if len(td) < 6: continue
        rows.append({"–°—Ç–∞–¥–∏–æ–Ω": td[1].a.text.strip(),
                     "–°—Å—ã–ª–∫–∞":  BASE+td[1].a["href"],
                     "–ü–æ—Å–µ—â–∞–µ–º–æ—Å—Ç—å": float(td[5].text.strip().replace(",",""))})
    return pd.DataFrame(rows)

def stadium_to_teams(link):
    return [a.text.strip() for a in _get(link).select("div.img16 a")]

def enrich(df):
    sd = stadium_table();  print("Parsing stadium pages ‚Ä¶")
    sd["–ö–æ–º–∞–Ω–¥—ã"] = sd["–°—Å—ã–ª–∫–∞"].apply(stadium_to_teams)
    st2teams = sd.set_index("–°—Ç–∞–¥–∏–æ–Ω")["–ö–æ–º–∞–Ω–¥—ã"].to_dict()
    st2att   = sd.set_index("–°—Ç–∞–¥–∏–æ–Ω")["–ü–æ—Å–µ—â–∞–µ–º–æ—Å—Ç—å"].to_dict()

    df["–î–æ–º–∞_1"] = df.apply(lambda r: int(r["–ö–æ–º–∞–Ω–¥–∞_1"] in st2teams.get(r["–°—Ç–∞–¥–∏–æ–Ω"], [])), axis=1)
    df["–î–æ–º–∞_2"] = df.apply(lambda r: int(r["–ö–æ–º–∞–Ω–¥–∞_2"] in st2teams.get(r["–°—Ç–∞–¥–∏–æ–Ω"], [])), axis=1)
    def guess(row):
        if pd.isna(row["–ó—Ä–∏—Ç–µ–ª–∏"]):
            base = st2att.get(row["–°—Ç–∞–¥–∏–æ–Ω"], np.nan)
            return np.nan if pd.isna(base) else int(base*(1+np.random.uniform(0.1,0.15)))
        return row["–ó—Ä–∏—Ç–µ–ª–∏"]
    df["–ó—Ä–∏—Ç–µ–ª–∏"] = df.apply(guess, axis=1);  return df

def download_csv(dest=SAVE_PATH):
    import subprocess
    print("Downloading snapshot ‚Ä¶")
    subprocess.run(["wget","-q","-O",dest,DROPBOX_URL], check=True)

if USE_DROPBOX:
    if not os.path.exists(SAVE_PATH): download_csv()
    df = pd.read_csv(SAVE_PATH, sep=";")
    print(f"Loaded {len(df)} rows from snapshot.")
else:
    if os.path.exists(SAVE_PATH):
        print("Removing old CSV to scrape fresh ‚Ä¶"); os.remove(SAVE_PATH)

    links = all_match_links()
    print(f"TOTAL links: {len(links)}")
    links = links[START_FROM:]
    recs  = []
    for i,u in enumerate(links, START_FROM+1):
        try:   recs.append(parse_match(u, i, START_FROM+len(links)))
        except Exception as e: print(f"‚ö† Skip {u}: {e}")

    df = enrich(pd.DataFrame(recs))

    drop_cols = ["URL","–°—Ç–∞–¥–∏–æ–Ω","–ê—Ç–∞–∫–∏_1","–ê—Ç–∞–∫–∏_2","–û–ø–∞—Å–Ω—ã–µ –∞—Ç–∞–∫–∏_1","–û–ø–∞—Å–Ω—ã–µ –∞—Ç–∞–∫–∏_2"]
    df.drop(columns=drop_cols, inplace=True, errors="ignore")

    for stat in ["xG","–û—Ç–±–æ—Ä—ã","–ü–µ—Ä–µ–¥–∞—á–∏","–¢–æ—á–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–¥–∞—á %","–ù–∞–≤–µ—Å—ã","–í–±—Ä–∞—Å—ã–≤–∞–Ω–∏—è"]:
        for side in (1,2):
            col = f"{stat}_{side}"
            df[col] = df.groupby(f"–ö–æ–º–∞–Ω–¥–∞_{side}")[col].transform(lambda x: x.fillna(x.mean()))

    df["–¢–∞—Ä–≥–µ—Ç"] = np.select([df["–ì–æ–ª—ã_1"]>df["–ì–æ–ª—ã_2"], df["–ì–æ–ª—ã_1"]<df["–ì–æ–ª—ã_2"]],[1,2],0)
    df.to_csv(SAVE_PATH, sep=";", index=False)
    print(f"Scrape complete ‚Äî saved {len(df)} rows to {SAVE_PATH}")

NON_NUM = {"–ö–æ–º–∞–Ω–¥–∞_1","–ö–æ–º–∞–Ω–¥–∞_2","–ü–æ–≥–æ–¥–∞","–î–∞—Ç–∞","–í—Ä–µ–º—è"}
def to_num(x): return (np.nan if pd.isna(x)
                       else pd.to_numeric(re.sub(r"[^\d.\-]","",str(x)), errors="coerce"))
for c in df.columns:
    if df[c].dtype=="object" and c not in NON_NUM:
        df[c] = df[c].apply(to_num).astype(float)

print(f"DataFrame ready {df.shape[0]} rows √ó {df.shape[1]} cols")
df.head()

import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

import re, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

NON_NUMERIC = {"–ö–æ–º–∞–Ω–¥–∞_1","–ö–æ–º–∞–Ω–¥–∞_2","–ü–æ–≥–æ–¥–∞","–î–∞—Ç–∞","–í—Ä–µ–º—è"}
def to_num(x):
    if pd.isna(x): return np.nan
    return pd.to_numeric(re.sub(r"[^\d.\-]","",str(x)), errors="coerce")

for c in df.columns:
    if df[c].dtype=="object" and c not in NON_NUMERIC:
        df[c] = df[c].apply(to_num).astype(float)

# correlation matrix (target excluded)
numeric_df = df.select_dtypes(include=np.number).drop(columns=["–¢–∞—Ä–≥–µ—Ç"])
plt.figure(figsize=(11,9))
print("Plotting correlation heat-map (target excluded)")
sns.heatmap(numeric_df.corr(), cmap="coolwarm", linewidths=.3, center=0)
plt.title("Correlation matrix (numeric features only)")
plt.tight_layout()
plt.show()

# leakage probe
corr_with_target = df.select_dtypes(include=np.number).corr()["–¢–∞—Ä–≥–µ—Ç"].drop("–¢–∞—Ä–≥–µ—Ç")
leak_candidates  = corr_with_target[ corr_with_target.abs() >= 0.8 ].sort_values(key=np.abs, ascending=False)

if leak_candidates.empty:
    print("\nNo severe leakage detected (|œÅ| ‚â• 0.8).")
else:
    print("\nPotential leakage features (|œÅ| ‚â• 0.8):")
    print(leak_candidates.to_string(float_format="%.3f"))

# modelling (drop leak features if any)
feat_cols = numeric_df.columns.difference(leak_candidates.index)
print(f"\nTraining RandomForest on {len(feat_cols)} features")
X, y = df[feat_cols], df["–¢–∞—Ä–≥–µ—Ç"]
Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=.25, stratify=y, random_state=42)

rf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)
rf.fit(Xtr, ytr)

print("\nClassification report")
print(classification_report(yte, rf.predict(Xte), target_names=["Draw","Home win","Away win"]))

# feature importances
imp = pd.Series(rf.feature_importances_, index=feat_cols).sort_values().tail(10)
plt.figure(figsize=(7,4))
print("Plotting top-10 feature importances")
imp.plot(kind="barh")
plt.xlabel("Relative importance")
plt.title("Top-10 RandomForest importances")
plt.tight_layout(); plt.show()

print("\nTop-10 importance values")
print(imp.sort_values(ascending=False).to_string(float_format="%.5f"))

import re, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

NON_NUMERIC = {"–ö–æ–º–∞–Ω–¥–∞_1","–ö–æ–º–∞–Ω–¥–∞_2","–ü–æ–≥–æ–¥–∞","–î–∞—Ç–∞","–í—Ä–µ–º—è"}
def to_num(x):
    if pd.isna(x): return np.nan
    return pd.to_numeric(re.sub(r"[^\d.\-]","",str(x)), errors="coerce")

for c in df.columns:
    if df[c].dtype=="object" and c not in NON_NUMERIC:
        df[c] = df[c].apply(to_num).astype(float)

#leakage detection
numeric_df = df.select_dtypes(include=np.number)
corr_target = numeric_df.corr()['–¢–∞—Ä–≥–µ—Ç'].drop('–¢–∞—Ä–≥–µ—Ç').abs().sort_values(ascending=False)

leak_cols = corr_target[corr_target > 0.8].index.tolist()
rule_based = [c for c in numeric_df.columns
              if re.match(r'–ì–æ–ª—ã_\d', c) or c.startswith('goal_diff')]
leak_cols = sorted(set(leak_cols + rule_based))

print("Suspected leakage features:")
print(leak_cols if leak_cols else "‚Äî none found with |œÅ| > 0.8")

# visualize correlations without leakage columns
plt.figure(figsize=(10,8))
sns.heatmap(numeric_df.drop(columns=leak_cols+['–¢–∞—Ä–≥–µ—Ç']).corr(),
            cmap='coolwarm', center=0, linewidths=.3)
plt.title("Correlation matrix (leakage features dropped)")
plt.tight_layout(); plt.show()


#  model training / evaluation
X = numeric_df.drop(columns=leak_cols + ['–¢–∞—Ä–≥–µ—Ç'])
y = numeric_df['–¢–∞—Ä–≥–µ—Ç'].astype(int)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=.25, stratify=y, random_state=42)

models = {
    "Random Forest": make_pipeline(
        SimpleImputer(strategy='mean'),
        RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)
    ),
    "Logistic Regression": make_pipeline(
        SimpleImputer(strategy='mean'),
        LogisticRegression(max_iter=1000, random_state=42)
    ),
    "XGBoost": make_pipeline(
        SimpleImputer(strategy='mean'),
        XGBClassifier(use_label_encoder=False, eval_metric='mlogloss',
                      n_estimators=300, random_state=42)
    ),
    "SVM RBF": make_pipeline(
        SimpleImputer(strategy='mean'),
        SVC(random_state=42)
    )
}

results = []
for name, model in models.items():
    print(f"Training {name} ‚Ä¶")
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    results.append({
        "Model": name,
        "Accuracy":  accuracy_score (y_test, pred),
        "Precision": precision_score(y_test, pred, average='macro', zero_division=0),
        "Recall":    recall_score  (y_test, pred, average='macro', zero_division=0),
        "F1 Score":  f1_score      (y_test, pred, average='macro', zero_division=0)
    })

results_df = (pd.DataFrame(results)
              .set_index("Model")
              .round(4)
              .sort_values("F1 Score", ascending=False))

print("\nModel comparison after leakage removal:")
display(results_df)

# top importances for Rand Forest
rf = models["Random Forest"]
feat_imp = pd.Series(rf[-1].feature_importances_, index=X.columns)
top10 = feat_imp.sort_values().tail(10)

plt.figure(figsize=(7,4))
top10.plot(kind='barh')
plt.title("Top-10 RF importances (leaks removed)")
plt.xlabel("importance"); plt.tight_layout(); plt.show()

NON_NUMERIC = {"–ö–æ–º–∞–Ω–¥–∞_1", "–ö–æ–º–∞–Ω–¥–∞_2", "–ü–æ–≥–æ–¥–∞", "–î–∞—Ç–∞", "–í—Ä–µ–º—è"}

def to_num(x):
    if pd.isna(x):
        return np.nan
    return pd.to_numeric(re.sub(r"[^\d.\-]", "", str(x)), errors="coerce")

for c in df.columns:
    if df[c].dtype == "object" and c not in NON_NUMERIC:
        df[c] = df[c].apply(to_num).astype(float)

numeric_df = df.select_dtypes(include=np.number)

# delete '–¢–∞—Ä–≥–µ—Ç' from cols
if '–¢–∞—Ä–≥–µ—Ç' in numeric_df.columns:
    numeric_df = numeric_df.drop(columns=['–¢–∞—Ä–≥–µ—Ç'])

corr_target = numeric_df.corrwith(df['–¢–∞—Ä–≥–µ—Ç']).abs().sort_values(ascending=False)
corr_cols = corr_target.head(10).index.tolist() + ["–¢–∞—Ä–≥–µ—Ç"]
print("Plotting correlation heatmap")
sns.heatmap(df[corr_cols].corr(), cmap="coolwarm", linewidths=.3, annot=True, fmt=".2f")
plt.title("Top 10 correlations")
plt.tight_layout()
plt.show()

num_cols = numeric_df.columns.tolist()  # list num values without –¢–∞—Ä–≥–µ—Ç

print("Training RandomForest")
X = numeric_df  #  –±–µ–∑ –¢–∞—Ä–≥–µ—Ç
y = df["–¢–∞—Ä–≥–µ—Ç"]

Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=.25, stratify=y, random_state=42)

rf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)
rf.fit(Xtr, ytr)

# Prediction
y_pred = rf.predict(Xte)

print("Classification report")
print(classification_report(yte, y_pred))

imp = pd.Series(rf.feature_importances_, index=X.columns).sort_values().tail(10)
print("Plotting feature importances")
imp.plot(kind="barh")
plt.xlabel("Importance")
plt.title("Top 10 feature importances")
plt.tight_layout()
plt.show()
print("Importance values")
print(imp.sort_values(ascending=False).to_string(float_format="%.5f"))

# @title
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

y_pred = rf.predict(Xte)

accuracy = accuracy_score(yte, y_pred)
precision = precision_score(yte, y_pred, average='weighted')
recall = recall_score(yte, y_pred, average='weighted')
f1 = f1_score(yte, y_pred, average='weighted')

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

# @title
import re, unicodedata, numpy as np, pandas as pd, matplotlib.pyplot as plt
from collections import Counter
from transformers import AutoTokenizer
from tqdm.auto import tqdm

MIN_APPEAR = 3
HOME_COL, AWAY_COL = "HomeGoals", "AwayGoals"   # –∏–∑ matches_raw

BERT_MODEL = "bert-base-multilingual-cased"
tokenizer  = AutoTokenizer.from_pretrained(BERT_MODEL)

def bert_words(text: str): #list of cleaned words

    toks = tokenizer.tokenize(text)
    out = []
    for tok in toks:
        tok = tok.lstrip("#").lower()
        tok = unicodedata.normalize("NFKD", tok)
        if re.fullmatch(r"[a-z–∞-—è\d]{2,}", tok, re.I):
            out.append(tok)
    return out

news = linked.copy()
HEADLINE_COL = "title" if "title" in news.columns else news.columns[0]

# —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã long-—Ñ–æ—Ä–º–∞—Ç
home = matches_raw.assign(team=matches_raw["HomeTeam"],
                          opp =matches_raw["AwayTeam"],
                          team_goals=matches_raw[HOME_COL],
                          opp_goals =matches_raw[AWAY_COL])
away = matches_raw.assign(team=matches_raw["AwayTeam"],
                          opp =matches_raw["HomeTeam"],
                          team_goals=matches_raw[AWAY_COL],
                          opp_goals =matches_raw[HOME_COL])
matches_long = pd.concat([home, away], ignore_index=True)[
    ["team","opp","team_goals","opp_goals"]
]

latest_match = (matches_long
                .drop_duplicates("team", keep="last")
                .set_index("team"))

def fetch_latest(team):
    if team in latest_match.index:
        r = latest_match.loc[team]
        return pd.Series([r.team_goals, r.opp_goals])
    return pd.Series([np.nan, np.nan])

news[["team_goals","opp_goals"]] = news["team"].apply(fetch_latest)
news[["team_goals","opp_goals"]] = news[["team_goals","opp_goals"]].apply(
    pd.to_numeric, errors="coerce"
)

#  BERT token list
tqdm.pandas(desc="BERT-tokenising")
news["bert_tokens"] = news[HEADLINE_COL].astype(str).progress_apply(bert_words)


news["tok_len"]   = news["bert_tokens"].apply(len)
news["goal_diff"] = news["team_goals"] - news["opp_goals"]
news["result_num"]= (news["goal_diff"]
                     .apply(lambda x: np.sign(x) if pd.notna(x) else pd.NA)
                     .astype("Int8"))

# win/loss stats
tok_win, tok_loss, tok_total = Counter(), Counter(), Counter()

for _, r in news.dropna(subset=["result_num"]).iterrows():
    toks = r["bert_tokens"]
    tok_total.update(toks)
    if r["result_num"] == 1:  tok_win.update(toks)
    elif r["result_num"]==-1: tok_loss.update(toks)

rows=[]
for tok, tot in tok_total.items():
    if tot < MIN_APPEAR: continue
    w, l = tok_win[tok], tok_loss[tok]
    rows.append(dict(token=tok,appearances=tot,wins=w,losses=l,
                     win_rate=round(w/tot,3),
                     loss_rate=round(l/tot,3),
                     win_minus_loss=round((w-l)/tot,3)))
tok_df = (pd.DataFrame(rows)
          .sort_values("win_minus_loss",ascending=False,na_position="last")
          .reset_index(drop=True))

print("\n=== Top BERT-tokens (wins vs losses) ===")
print(tok_df.head(20).to_string(index=False))

tok_score_map = tok_df.set_index("token")["win_minus_loss"].to_dict()
def headline_tok_score(toks):
    return float(np.mean([tok_score_map.get(t,0) for t in toks])) if toks else 0.0
news["tok_score"] = news["bert_tokens"].apply(headline_tok_score)

#  correlation
num_cols = ["sentiment","abs_sent","tok_len","tok_score","goal_diff","result_num"]
news[num_cols] = news[num_cols].apply(pd.to_numeric, errors="coerce").astype(float)
corr_input = news[num_cols].dropna(thresh=2)
corr = (corr_input.corr(min_periods=2).round(3)
        if not corr_input.empty else
        pd.DataFrame(index=num_cols, columns=num_cols))

print("\n=== Correlation matrix (BERT) ===")
print(corr.fillna("").to_string())

if not corr_input.empty and not corr.isna().all().all():
    fig, ax = plt.subplots(figsize=(7,6))
    im = ax.imshow(corr, vmin=-1, vmax=1, cmap="coolwarm")
    ax.set_xticks(range(len(corr)), labels=corr.columns, rotation=45, ha="right")
    ax.set_yticks(range(len(corr)), labels=corr.index)
    for i in range(len(corr)):
        for j in range(len(corr)):
            txt = "" if pd.isna(corr.iat[i,j]) else f"{corr.iat[i,j]:+.2f}"
            ax.text(j, i, txt, ha="center", va="center", fontsize=8)
    ax.set_title("Correlation matrix (BERT tokens)")
    fig.colorbar(im, shrink=0.8)
    plt.tight_layout(); plt.show()
else:
    print("\n correlation matrix empty.")